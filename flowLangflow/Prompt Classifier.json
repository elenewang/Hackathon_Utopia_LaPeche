{"id":"a176ce74-1008-415d-aaf1-bcbe28df2119","data":{"nodes":[{"id":"TextInput-ghjCa","type":"genericNode","position":{"x":697.9500229854227,"y":417.587256171706},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"https://secure.booking.com/","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Text Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"TextInput","id":"TextInput-ghjCa"},"selected":false,"width":320,"height":233,"positionAbsolute":{"x":697.9500229854227,"y":417.587256171706},"dragging":false},{"id":"Prompt-vkQn4","type":"genericNode","position":{"x":1040.1545394446434,"y":357.2867708664356},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{website_name}\n\nIdentify the company the above website belongs to.\nClassify the company as Transport, Ecommerce, Insurance, Company that offers a Subscription plan or something else. \nReturn a single word as output from the following options that closely relates to your classification of the company.\n1. Transport\n2. Ecommerce\n3. Insurance\n4. Subscription \n5. Other","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"website_name":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"website_name","display_name":"website_name","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["website_name"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"legacy":false,"error":null,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"Prompt","id":"Prompt-vkQn4"},"selected":false,"width":320,"height":346,"dragging":false,"positionAbsolute":{"x":1040.1545394446434,"y":357.2867708664356}},{"id":"MistralModel-SzYBV","type":"genericNode","position":{"x":1386.509626126721,"y":284.50417730255776},"data":{"node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":false,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"Mistral API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The Mistral API Key to use for the Mistral model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_mistralai import ChatMistralAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import BoolInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\n\n\nclass MistralAIModelComponent(LCModelComponent):\n    display_name = \"MistralAI\"\n    description = \"Generates text using MistralAI LLMs.\"\n    icon = \"MistralAI\"\n    name = \"MistralModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=[\n                \"open-mixtral-8x7b\",\n                \"open-mixtral-8x22b\",\n                \"mistral-small-latest\",\n                \"mistral-medium-latest\",\n                \"mistral-large-latest\",\n                \"codestral-latest\",\n            ],\n            value=\"codestral-latest\",\n        ),\n        StrInput(\n            name=\"mistral_api_base\",\n            display_name=\"Mistral API Base\",\n            advanced=True,\n            info=\"The base URL of the Mistral API. Defaults to https://api.mistral.ai/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Mistral API Key\",\n            info=\"The Mistral API Key to use for the Mistral model.\",\n            advanced=False,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", advanced=False, value=0.5),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", advanced=True, value=5),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", advanced=True, value=60),\n        IntInput(name=\"max_concurrent_requests\", display_name=\"Max Concurrent Requests\", advanced=True, value=3),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", advanced=True, value=1),\n        IntInput(name=\"random_seed\", display_name=\"Random Seed\", value=1, advanced=True),\n        BoolInput(name=\"safe_mode\", display_name=\"Safe Mode\", advanced=True),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        mistral_api_key = self.api_key\n        temperature = self.temperature\n        model_name = self.model_name\n        max_tokens = self.max_tokens\n        mistral_api_base = self.mistral_api_base or \"https://api.mistral.ai/v1\"\n        max_retries = self.max_retries\n        timeout = self.timeout\n        max_concurrent_requests = self.max_concurrent_requests\n        top_p = self.top_p\n        random_seed = self.random_seed\n        safe_mode = self.safe_mode\n\n        api_key = SecretStr(mistral_api_key).get_secret_value() if mistral_api_key else None\n\n        return ChatMistralAI(\n            max_tokens=max_tokens or None,\n            model_name=model_name,\n            endpoint=mistral_api_base,\n            api_key=api_key,\n            temperature=temperature,\n            max_retries=max_retries,\n            timeout=timeout,\n            max_concurrent_requests=max_concurrent_requests,\n            top_p=top_p,\n            random_seed=random_seed,\n            safe_mode=safe_mode,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_concurrent_requests":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_concurrent_requests","value":3,"display_name":"Max Concurrent Requests","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"max_retries":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_retries","value":5,"display_name":"Max Retries","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"mistral_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"mistral_api_base","value":"","display_name":"Mistral API Base","advanced":true,"dynamic":false,"info":"The base URL of the Mistral API. Defaults to https://api.mistral.ai/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["open-mixtral-8x7b","open-mixtral-8x22b","mistral-small-latest","mistral-medium-latest","mistral-large-latest","codestral-latest"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"mistral-small-latest","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"random_seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"random_seed","value":1,"display_name":"Random Seed","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"safe_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"safe_mode","value":false,"display_name":"Safe Mode","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool","_input_type":"BoolInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":false,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"Reply in a single word","display_name":"System Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"},"timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"timeout","value":60,"display_name":"Timeout","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_p","value":1,"display_name":"Top P","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using MistralAI LLMs.","icon":"MistralAI","base_classes":["LanguageModel","Message"],"display_name":"MistralAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_tokens","model_name","mistral_api_base","api_key","temperature","max_retries","timeout","max_concurrent_requests","top_p","random_seed","safe_mode","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"MistralModel","id":"MistralModel-SzYBV"},"selected":false,"width":320,"height":720,"dragging":false,"positionAbsolute":{"x":1386.509626126721,"y":284.50417730255776}},{"id":"ConditionalRouter-qulq5","type":"genericNode","position":{"x":2113.6539733075388,"y":233.697223179829},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MessageInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass ConditionalRouterComponent(Component):\n    display_name = \"Switch\"\n    description = \"Routes an input message to a corresponding output based on text comparison.\"\n    icon = \"split\"\n    name = \"ConditionalRouter\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.__iteration_updated = False\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Text Input\",\n            info=\"The primary text input for the operation.\",\n        ),\n        MessageTextInput(\n            name=\"transport_prompt\",\n            display_name=\"Transport Prompt Input\",\n        ),\n        MessageTextInput(\n            name=\"ecommerce_prompt\",\n            display_name=\"Ecommcerce Prompt Input\",\n        ),\n        MessageTextInput(\n            name=\"insurance_prompt\",\n            display_name=\"Insurance Prompt Input\",\n        ),\n        MessageTextInput(\n            name=\"subscription_prompt\",\n            display_name=\"Subscription Prompt Input\",\n        ),\n        MessageTextInput(\n            name=\"general_prompt\",\n            display_name=\"General Prompt Input\",\n        ),\n        MessageInput(\n            name=\"message\",\n            display_name=\"Message\",\n            info=\"The message to pass through either route.\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt\", name=\"prompt_result\", method=\"prompt_response\"),\n    ]\n\n    def _pre_run_setup(self):\n        self.__iteration_updated = False\n\n    def evaluate_condition(self, input_text: str, match_text: str) -> bool:\n        input_text = input_text.lower()\n        match_text = match_text.lower()\n        return input_text == match_text\n\n    def iterate_and_stop_once(self, route_to_stop: str):\n        if not self.__iteration_updated:\n            self.update_ctx({f\"{self._id}_iteration\": self.ctx.get(f\"{self._id}_iteration\", 0) + 1})\n            self.__iteration_updated = True\n            if self.ctx.get(f\"{self._id}_iteration\", 0) >= 1 and route_to_stop == self.default_route:\n                # We need to stop the other route\n                route_to_stop = \"true_result\" if route_to_stop == \"false_result\" else \"false_result\"\n            self.stop(route_to_stop)\n\n    def prompt_response(self) -> Message:\n        if(self.evaluate_condition(self.input_text, \"Transport\")):\n            self.status = self.message\n            return self.transport_prompt\n        elif(self.evaluate_condition(self.input_text, \"Ecommerce\")):\n            self.status = self.message\n            return self.ecommerce_prompt\n        elif(self.evaluate_condition(self.input_text, \"Insurance\")):\n            self.status = self.message\n            return self.insurance_prompt\n        elif(self.evaluate_condition(self.input_text, \"Subscription\")):\n            self.status = self.message\n            return self.subscription_prompt\n        else:\n            self.status = self.message\n            return self.general_prompt\n        \n            # self.iterate_and_stop_once(\"false_result\")\n            \n\n\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"ecommerce_prompt":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"ecommerce_prompt","value":"","display_name":"Ecommcerce Prompt Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"general_prompt":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"general_prompt","value":"","display_name":"General Prompt Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"input_text":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_text","value":"","display_name":"Text Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The primary text input for the operation.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"insurance_prompt":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"insurance_prompt","value":"","display_name":"Insurance Prompt Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"message","value":"","display_name":"Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The message to pass through either route.","title_case":false,"type":"str","_input_type":"MessageInput"},"subscription_prompt":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"subscription_prompt","value":"","display_name":"Subscription Prompt Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"transport_prompt":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"transport_prompt","value":"","display_name":"Transport Prompt Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Routes an input message to a corresponding output based on text comparison.","icon":"split","base_classes":["Message"],"display_name":"If-Else","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt_result","display_name":"Prompt","method":"prompt_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_text","transport_prompt","ecommerce_prompt","insurance_prompt","subscription_prompt","general_prompt","message"],"beta":false,"legacy":false,"edited":true,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"ConditionalRouter","id":"ConditionalRouter-qulq5"},"selected":false,"width":320,"height":687,"positionAbsolute":{"x":2113.6539733075388,"y":233.697223179829},"dragging":false},{"id":"Prompt-sYdnJ","type":"genericNode","position":{"x":1727.38071035951,"y":-19.238718747246708},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Assume the role of an expert in consumer legal matters in the Transport domain\n\nDoes the policy allow the user to get a refund if they cancel the trip in advance?\n\nIs the baggage allowance clearly stated and free for a reasonable weight (e.g. 20kg)?\n\nDoes the company offer compensation (e.g. food, hotel) for delays or cancellations?\n\nAre missed connections or rebooking explained and covered?\n\nIs the refund processing time mentioned and limited (e.g. under 10 days)?\n\n----------------------------------------------------------------\nIf some of the questions don't apply or is not relevant to this company, then don't answer it.\nClassify the above answers into good and bad and reply as a JSON\n","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":[]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"legacy":false,"error":null,"edited":false,"metadata":{},"tool_mode":false},"type":"Prompt","id":"Prompt-sYdnJ"},"selected":false,"width":320,"height":259,"dragging":false,"positionAbsolute":{"x":1727.38071035951,"y":-19.238718747246708}},{"id":"Prompt-4A0Oa","type":"genericNode","position":{"x":1739.84768194449,"y":265.47728381579697},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Assume the role of an expert in consumer legal matters in the Ecommerce domain\n\nCan I return or exchange items easily?\n(Checks if the policy allows returns/refunds and under what conditions.)\n\nWhat if my order is lost, late, or damaged?\n(Finds info on customer protections for failed deliveries or product issues.)\n\nAre there any hidden fees I should know about?\n(Scans for surprise charges like extra taxes, shipping, or service fees.)\n\n Is my personal and payment data secure?\n(Looks for mentions of encryption, third-party sharing, and privacy practices.)\n\n Will I be auto-subscribed or auto-billed?\n(Flags recurring charges, free trial traps, and opt-out clarity.)\n\nCan they change prices, products, or terms without warning?\n(Highlights if the company can alter key terms unilaterally.)\n\nDo I keep my rewards, points, or credits if I stop using the site?\n(Checks if loyalty perks or balances can expire or be revoked.)\n\n----------------------------------------------------------------\nIf some of the questions don't apply or is not relevant to this company, then don't answer it.\nClassify the above answers into good and bad and reply as a JSON\n","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":[]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"legacy":false,"error":null,"edited":false,"metadata":{},"tool_mode":false},"type":"Prompt","id":"Prompt-4A0Oa"},"selected":false,"width":320,"height":259,"positionAbsolute":{"x":1739.84768194449,"y":265.47728381579697},"dragging":false},{"id":"Prompt-rBnzc","type":"genericNode","position":{"x":1739.7393842780693,"y":562.1771928073273},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Assume the role of an expert in consumer legal matters in the Insurance domain\n\nWhat’s actually covered—and what’s not?\n(Checks for clear coverage details and exclusions, so users know what they’re protected against.)\n\nWhen and how can I file a claim?\n(Scans for claim eligibility, deadlines, required documents, and filing process transparency.)\n\nCan they cancel or change my policy without telling me?\n(Flags clauses allowing the insurer to modify terms, pricing, or cancel coverage.)\n\nAre there any hidden fees or automatic renewals?\n(Looks for policy renewal traps, processing fees, or charges that aren't clearly explained.)\n\nHow long do payouts or reimbursements usually take?\n(Identifies sections discussing claim settlement timeframes or potential delays.)\n\nCan I cancel my policy, and will I get a refund?\n(Checks for cancellation rights, penalties, and refund eligibility.)\n\nHow is my personal and medical information used?\n(Surfaces data sharing practices—especially with third parties or marketing partners.)\n\n----------------------------------------------------------------\nIf some of the questions don't apply or is not relevant to this company, then don't answer it.\nClassify the above answers into good and bad and reply as a JSON\n","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":[]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"legacy":false,"error":null,"edited":false,"metadata":{},"tool_mode":false},"type":"Prompt","id":"Prompt-rBnzc"},"selected":false,"width":320,"height":259,"positionAbsolute":{"x":1739.7393842780693,"y":562.1771928073273},"dragging":false},{"id":"Prompt-N62CL","type":"genericNode","position":{"x":1739.486573565614,"y":860.0587244062679},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Assume the role of an expert in consumer legal matters in the Subscription based companies like Netflix, Spotify and the like\n\nDoes the policy notify the user before auto-renewing the subscription?\n\nCan the user cancel the subscription at any time without penalty?\n\nIs the user warned before a free trial converts to a paid plan?\n\nAre users eligible for a refund (full or partial) after cancellation?\n\nAre price changes announced in advance before they apply?\n\n----------------------------------------------------------------\nIf some of the questions don't apply or is not relevant to this company, then don't answer it.\nClassify the above answers into good and bad and reply as a JSON\n","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":[]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"legacy":false,"error":null,"edited":false,"metadata":{},"tool_mode":false},"type":"Prompt","id":"Prompt-N62CL"},"selected":false,"width":320,"height":259,"positionAbsolute":{"x":1739.486573565614,"y":860.0587244062679},"dragging":false},{"id":"Prompt-BXBIl","type":"genericNode","position":{"x":1750.0556883238496,"y":1164.3745303663804},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Assume the role of an expert in consumer legal matters\n\nDoes the policy state that user data will not be sold to third parties?\n\nCan users request deletion of their data at any time?\n\nIs location tracking disabled by default or only used with consent?\n\nDo users retain ownership of the content they upload or share?\n\nCan users take legal action without being forced into arbitration?\n\n----------------------------------------------------------------\nIf some of the questions don't apply or is not relevant to this company, then don't answer it.\nClassify the above answers into good and bad and reply as a JSON\n","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":[]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"legacy":false,"error":null,"edited":false,"metadata":{},"tool_mode":false},"type":"Prompt","id":"Prompt-BXBIl"},"selected":false,"width":320,"height":259,"positionAbsolute":{"x":1750.0556883238496,"y":1164.3745303663804},"dragging":false},{"id":"ChatOutput-80qsZ","type":"genericNode","position":{"x":2465.788968266856,"y":442.662467527153},"data":{"node":{"template":{"_type":"Component","background_color":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"background_color","value":"","display_name":"Background Color","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The background color of the icon.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"chat_icon":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"chat_icon","value":"","display_name":"Icon","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The icon of the message.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, _id: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if _id:\n            source_dict[\"id\"] = _id\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            source_dict[\"source\"] = source\n        return Source(**source_dict)\n\n    def message_response(self) -> Message:\n        _source, _icon, _display_name, _source_id = self.get_properties_from_source_component()\n        _background_color = self.background_color\n        _text_color = self.text_color\n        if self.chat_icon:\n            _icon = self.chat_icon\n        message = self.input_value if isinstance(self.input_value, Message) else Message(text=self.input_value)\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(_source_id, _display_name, _source)\n        message.properties.icon = _icon\n        message.properties.background_color = _background_color\n        message.properties.text_color = _text_color\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"data_template","value":"{text}","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str","_input_type":"MessageInput"},"sender":{"tool_mode":false,"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"AI","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"},"text_color":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"text_color","value":"","display_name":"Text Color","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The text color of the name","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Display a chat message in the Playground.","icon":"MessagesSquare","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template","background_color","chat_icon","text_color"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"ChatOutput","id":"ChatOutput-80qsZ"},"selected":true,"width":320,"height":233,"positionAbsolute":{"x":2465.788968266856,"y":442.662467527153},"dragging":false}],"edges":[{"source":"TextInput-ghjCa","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-ghjCaœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-vkQn4","targetHandle":"{œfieldNameœ:œwebsite_nameœ,œidœ:œPrompt-vkQn4œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"website_name","id":"Prompt-vkQn4","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-ghjCa","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-ghjCa{œdataTypeœ:œTextInputœ,œidœ:œTextInput-ghjCaœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-vkQn4{œfieldNameœ:œwebsite_nameœ,œidœ:œPrompt-vkQn4œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-vkQn4","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-vkQn4œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"MistralModel-SzYBV","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œMistralModel-SzYBVœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"MistralModel-SzYBV","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-vkQn4","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-vkQn4{œdataTypeœ:œPromptœ,œidœ:œPrompt-vkQn4œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-MistralModel-SzYBV{œfieldNameœ:œinput_valueœ,œidœ:œMistralModel-SzYBVœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"MistralModel-SzYBV","sourceHandle":"{œdataTypeœ:œMistralModelœ,œidœ:œMistralModel-SzYBVœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ConditionalRouter-qulq5","targetHandle":"{œfieldNameœ:œinput_textœ,œidœ:œConditionalRouter-qulq5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_text","id":"ConditionalRouter-qulq5","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"MistralModel","id":"MistralModel-SzYBV","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-MistralModel-SzYBV{œdataTypeœ:œMistralModelœ,œidœ:œMistralModel-SzYBVœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ConditionalRouter-qulq5{œfieldNameœ:œinput_textœ,œidœ:œConditionalRouter-qulq5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-sYdnJ","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-sYdnJœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"ConditionalRouter-qulq5","targetHandle":"{œfieldNameœ:œtransport_promptœ,œidœ:œConditionalRouter-qulq5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"transport_prompt","id":"ConditionalRouter-qulq5","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-sYdnJ","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-sYdnJ{œdataTypeœ:œPromptœ,œidœ:œPrompt-sYdnJœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-ConditionalRouter-qulq5{œfieldNameœ:œtransport_promptœ,œidœ:œConditionalRouter-qulq5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-4A0Oa","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-4A0Oaœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"ConditionalRouter-qulq5","targetHandle":"{œfieldNameœ:œecommerce_promptœ,œidœ:œConditionalRouter-qulq5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"ecommerce_prompt","id":"ConditionalRouter-qulq5","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-4A0Oa","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-4A0Oa{œdataTypeœ:œPromptœ,œidœ:œPrompt-4A0Oaœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-ConditionalRouter-qulq5{œfieldNameœ:œecommerce_promptœ,œidœ:œConditionalRouter-qulq5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-rBnzc","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-rBnzcœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"ConditionalRouter-qulq5","targetHandle":"{œfieldNameœ:œinsurance_promptœ,œidœ:œConditionalRouter-qulq5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"insurance_prompt","id":"ConditionalRouter-qulq5","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-rBnzc","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-rBnzc{œdataTypeœ:œPromptœ,œidœ:œPrompt-rBnzcœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-ConditionalRouter-qulq5{œfieldNameœ:œinsurance_promptœ,œidœ:œConditionalRouter-qulq5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-N62CL","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-N62CLœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"ConditionalRouter-qulq5","targetHandle":"{œfieldNameœ:œsubscription_promptœ,œidœ:œConditionalRouter-qulq5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"subscription_prompt","id":"ConditionalRouter-qulq5","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-N62CL","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-N62CL{œdataTypeœ:œPromptœ,œidœ:œPrompt-N62CLœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-ConditionalRouter-qulq5{œfieldNameœ:œsubscription_promptœ,œidœ:œConditionalRouter-qulq5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-BXBIl","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-BXBIlœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"ConditionalRouter-qulq5","targetHandle":"{œfieldNameœ:œgeneral_promptœ,œidœ:œConditionalRouter-qulq5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"general_prompt","id":"ConditionalRouter-qulq5","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-BXBIl","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-BXBIl{œdataTypeœ:œPromptœ,œidœ:œPrompt-BXBIlœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-ConditionalRouter-qulq5{œfieldNameœ:œgeneral_promptœ,œidœ:œConditionalRouter-qulq5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"ConditionalRouter-qulq5","sourceHandle":"{œdataTypeœ:œConditionalRouterœ,œidœ:œConditionalRouter-qulq5œ,œnameœ:œprompt_resultœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-80qsZ","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-80qsZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-80qsZ","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ConditionalRouter","id":"ConditionalRouter-qulq5","name":"prompt_result","output_types":["Message"]}},"id":"reactflow__edge-ConditionalRouter-qulq5{œdataTypeœ:œConditionalRouterœ,œidœ:œConditionalRouter-qulq5œ,œnameœ:œprompt_resultœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-80qsZ{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-80qsZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""}],"viewport":{"x":-219.56966226967256,"y":44.11388916621479,"zoom":0.4651389534029119}},"description":"Classify prompts based on domain of the company by getting a look of their input","name":"Prompt Classifier","last_tested_version":"1.1.0","endpoint_name":null,"is_component":false}